In order to test our design, two virtualization stacks, a test infrastructure, and several benchmarks were created.  Our test suite runs a load on the guest machines and coordinates the stop and start of the collection of multiple performance counters across the guests and hypervisor.     
Our experiments, when run in this infrastructure, demonstrates performance problems guest machines may experience, from external interference or changes to the application workload.  The performance counters are collected from all layers of the virtualization stack and the results of the application performance are also recorded.  For each experiment, we run the test at least three times and average the results.

Both virtualization platforms have the same software stack installed (Figure \ref{softStack}).  
We use the Xen hypervisor and CentOS release for both the guest (DomU) and VMM (Dom0).  
This software stack can be extended to mulptiple servers to produce a larger cluster of servers.  
We chose PostgreSQL as the database server for our tests because of its robustness and standard use in hosting facilities and applications.  
It is a good general purpose application that can be used to stress I/O, memory, and CPU resources, although we focused our test suite to measure database read operations.

The test suite can run in all sizes of virtualization environments (Table \ref{virtSize}).  
Tests which run in a \emph{Personal} or \emph{Business} virtualization platform can also be run in a \emph{Cluster} or \emph{Cloud} based system by increasing the size of the database or number of guest machines until a similar load is reached.  
For our experiments we run on an IBM x3650 \emph{Personal} and Dell T410 \emph{Business} sized virtualization environments. 

\begin{table}[h]
\begin{subtable}[h]{0.45\textwidth}
\begin{tabular}{ l p{5cm} }
  Software & Version \\
  \hline
  Hyperviser & Xen 4.2 \\
  Domain 0 & CentOS 6.5 (Kernel 3.4) \\
  Guest Domains & CentOS 6.4 (Kernel 2.6.39) PostgreSQL 8.4 \\
  \hline
\end{tabular}
\caption{Software installed virtualization test stack}
\label{softStack}
\end{subtable}
\hfill
\begin{subtable}[h]{0.45\textwidth}
\begin{tabular}{ l p{5cm} }
  Size & Specifications \\
  \hline
  Personal & IBM x3650 Quad Core 2GB Ram \\
  Business & Dell PowerEdge T410 dual quad-core Xeon processors, 12GB Ram. \\
  Cluster & Multiple small or medium servers clustered together with shared SAN data store. \\
  Cloud & Amazon Cloud or similar PAAS provider. \\
  \hline
\end{tabular}
\caption{Virtualization sizes for tests}
\label{virtSize}
\end{subtable}
\caption{Software and Hardware stacks}
\end{table}

The test infrastructure generates a consistent and reproducible system load by using a PostgreSQL database server with PGbench.  
 PGbench creates a TPC-B similar style workload and calculates transactions per second (TPS), so we can track the performance of each guest system from the application layer \cite{pgTune}.  
 We use PGbench to both generate a load to measure overhead and create interference.  
 We can change the database size and the number of clients that connect to the database to change the I/O workload from the guest perspective.
PGbench will put a stress load on the limits of the system, and it is a good tool for simulating guest machines that are all consuming as much resources as possible.  
 We measure the TPS from the benchmark when run as a single virtual machine and when run with multiple machines concurrently.  We compare the degradation from the application to our proposed calculation for interference.

When the test begins, each guest VM starts a process which waits for the hypervisor to signal the start of the test.  
 The Dom0 (VMM) then starts a process which signals all the guests to start, and which benchmark to run.  
 Each guest then collects the $pre$ resource counters and begins running the specified benchmark system load.  
 Dom0 also collects the same resource counters from its view, but does not generate a load, since in Xen all disk I/O goes through Dom0, we use this as the hypervisor layer.  
 When each guest completes the benchmark, it reads the $post$ counters and sends the difference between the two counters back to Dom0 (Figure \ref{alg1}).
 When Dom0 has collected the information from all guests, it can display the counters for itself and each guest.
 Executing this once with a single virtual machine, and once with multiple virtual machines we can calculate the interference.

One of the challenges of this test suite is the coordination of the starting and stopping of the test, collection of resource counters, and the information sharing between the guests and hypervisor.  
Since we use Xen as our virtualization platform, we chose XenBus and XenStore \cite{xenbus} to both coordinate the stop and start of the benchmark, and the collection of data results between domains.
In order for paravirtualized guest domains to communicate with the hypervisor the XenStore tools for DomU domains had to be built as this is not currently part of the standard package of guest tools.  Other options were to use the TCP/IP stack to send and receive messages between the domains.


We can use the TPS to show how the database performs at the application layer when external environment changes are made to the system.  Our tests will select Memory and I/O resources to monitor.  For measuring disk I/O it is a good to also collect virtual memory (in this case we are talking about OS virtual memory, not virtualization) as well as I/O data.  Our infrastructure collects paging performance counters and disk read counters at all layers to measure interference from I/O. 

