In order to test our design, two virtualization stacks, a test infrastructure, and several benchmarks were created.  Our test suite runs load on the guest machines and coordinate the stop and start of the collection of multiple performance counters.     
Our experiments, when run in this infrastructure, demonstrate the performance problems guest machines may experience when running the same system load while running by itself and with multiple machines running simultaneously.  

\subsection{Methodology}
Each guest VM in the test starts a process and waits for the hypervisor to signal the start of the test.  
The hypervisor then starts a process which signals all the guests to start, and which benchmark to run.  
Each guest then collects the $pre$ resource counters and begins running the specified benchmark system load.  
The hypervisor also collects the same resource counters from its view, but does not generate a load.  
When each guest completes the benchmark, it reads the $post$ counters and sends the difference between the two counters back to the hypervisor (Figure \ref{alg1}).
When the hypervisor has collected the information from all guests, it then calculates the overheard (Equation 1 or 2).
By running this scenario once with only 1 virtual machine, and once with multiple virtual machines we can calculate the interference (Equation 3).

\subsection{Software and Hardware}
\indent Both virtualization platforms have the same software stack installed (Figure \ref{fig:softStack}).  
We use the Xen hypervisor and CentOS release for both the guest (DomU) and VMM (Dom0).  
This software stack can be extended to mulptiple servers to produce a larger cluster of servers.  
We chose Postgres as the database server for our tests because of its robustness and standard use in hosting facilities and applications.  
It is a good general purpose application that can be used to stress I/O, memory, and CPU resources, although we focused our test suite to measure database read operations.

\indent The tests can be run in all sizes of virtualization environments (Figure \ref{fig:virtSize}).  
Tests which run in a \emph{Small} or \emph{Medium} environment can also be run in a \emph{Cluster} or \emph{Cloud} based system by increasing the size of the database or number of guest machines until a similar load is reached.  
For our experiments we run on a \emph{Small} and \emph{Medium} sized virtualization environments. 

\begin{figure}[lh]
\begin{tabular}{ l p{5cm} }
  Software & Version \\
  \hline
  Hyperviser & Xen 4.2 \\
  Domain 0 & CentOS 6.2 (Kernel 3.4) \\
  Guest Domains & CentOS 6.2 (Kernel 2.6.39) PostgreSQL 8.4 \\
  \hline
\end{tabular}
\caption{Software installed virtualization test stack}
\label{fig:softStack}
\end{figure}

\begin{figure}[rh]
\begin{tabular}{ l p{5cm} }
  Size & Specifications \\
  \hline
  Small & IBM x3650 Quad Core 2GB Ram \\
  Medium & Dell PowerEdge T410 dual quadcore Xeon processors, 8GB Ram. \\
  Cluster & Multiple small or medium servers clustered together with shared SAN data store. \\
  Cloud & Amazon Cloud or similar PAAS provider. \\
  \hline
\end{tabular}
\caption{Virtualization sizes for tests}
\label{fig:virtSize}
\end{figure}

\indent The test infrasture generates a consistent and reproducible system load by using a PostgreSQL database server with PGbench and PGreplay. 
PGbench creates a TPC-B similar style workload and calculates transactions per second (TPS), so we can track the performance of each guest system from the application layer \cite{pgTune}.  
We use PGbench to both generate a load to measure overhead and create interference.
PGreplay is used to record and playback (faster or slower) the same set of benchmark transactions, which allows us to simulate host systems that are not running at 100\% capacity.  
Instead of running multiple guests each with a fraction of the resources, we can overcommit the resources, generate a smaller load, and measure the interference from overcommitment.
This is typical of most virtual systems, since at almost all times, the systems are using only a fraction of the total resources available.  
The test suite infrastructure will allow any benchmark or command to run on the systems as the command to run is passed from the hypervisor to the guest machines.
Several other benchmarks such as Postmark \cite{katcher} were also considered, and could be easily used in this test framework.

\subsection{Domain Communication}
\indent One of the challenges of this test suite is the coordination of the starting and stopping of the test, collection of resource counters, and the information sharing between the guests and hypervisor.  
Since we use Xen as our virtualization platform, we chose XenBus and XenStore \cite{xenbus}. 
XenBus provides an abstraction for paravirtualized drivers to communicate between domains.  
XenStore is an information storage space shared between domains.  
These tools are often used to configure and manage domains with higher level user tools, and can be configured to provide security between domains.
In order for paravitualized guest domains to communicate with the hypervisor the XenStore tools for DomU domains had to be built as this is not currently part of the standard package of guest tools.  Other options were to use the TCP/IP stack to send and receive messages between the domains to coordinate the tests and information as well.

\subsection{I/O Workload}
For I/O intensive workloads, the application tends to perform very well when the entire \emph{working set} can fit into main memory.  
However, when the \emph{working set}  approaches (or exceeds) the size of main memory, the application tends to degrade quickly.  
Our initial experiments to find an I/O workload highlight this fact by changing the database size under load in a virtual environment.
Before running the entire test suite we need to create a test database that will exceed the \emph{working set} and increase the probability that a database read will need to fetch the data from disk storage.  With PGbench the "-i" flag \emph{scaling factor} is used to initialize a database at a specified size so that our results can show changes between a memory bound system and I/O bound system.  
A script runs and creates a small database then runs a benchmark and collects the results of the benchmark - TPS.  The script then creates a slighly larger database and runs the benchmark again.  This process is repeated until the TPS drop significantly, and the CPU time is almost idle.  At this point we can determine an adequate database size for an IO bound system.

\subsection{Test Data Collection}
\indent For each test result, at least 3 trials are completed in each configuration.  The average results of these test are measured in \textbf{TPS} \emph{Transactions per Second}, and performance counters captured at each layer.   
We can use the TPS to show how the database performs at the application layer when external environment changes are made to the system.  We use the performance counters to try to analyze the interference and explain the degredation from virtualization.

\begin{table}[h]
\begin{subtable}[h]{0.45\textwidth}
\begin{tabular}{ l p{5cm} }
       measurement & description \\
       \hline
       swpd  & the amount of virtual memory used. \\
       free  & the amount of idle memory. \\
       buff  & the amount of memory used as buffers \\
       cache & the amount of memory used as cache \\
       inact & the amount of inactive memory. \\
       active & the amount of active memory. \\
       \hline
\end{tabular}
\caption{Virtual memory data collected}
\label{fig:memory}
\end{subtable}
\hfill
\begin{subtable}[h]{0.45\textwidth}
\begin{tabular}{ l p{5cm} }
       measurement & description \\
       \hline
       si & Amount of memory swapped in from disk (/s). \\
       so & Amount of memory swapped to disk (/s). \\
       bi & Blocks received from a block device (blocks/s). \\
       bo & Blocks sent to a block device (blocks/s). \\
       wa & Time spent waiting for IO. \\
       \hline
\end{tabular}
\caption{I/O read data collected}
\label{fig:io}
\end{subtable}
\caption{Test result data collected from guest and hypervisor}
\end{table}

