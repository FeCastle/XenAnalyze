In order to test our design, two virtualization stacks, a test infrastructure, and several benchmarks were created.  Our test suite runs a load on the guest machines and coordinates the stop and start of the collection of multiple performance counters across the guests and hypervisor.     
Our experiments, when run in this infrastructure, demonstrate the performance problems guest machines may experience.  We run the benchmark at least three times with the guest isolated and at least three times with multiple guests running simultaneously.  We report the average results of these tests.

\subsection{Methodology}
Firts, each guest VM starts a process and waits for the hypervisor to signal the start of the test.  
The hypervisor then starts a process which signals all the guests to start, and which benchmark to run.  
Each guest then collects the $pre$ resource counters and begins running the specified benchmark system load.  
The hypervisor also collects the same resource counters from its view, but does not generate a load.  
When each guest completes the benchmark, it reads the $post$ counters and sends the difference between the two counters back to the hypervisor (Figure \ref{alg1}).
When the hypervisor has collected the information from all guests, it then calculates the overheard (Equation 1 or 2).
By running this scenario once with only 1 virtual machine, and once with multiple virtual machines we can calculate the interference for those counters that measure the time spent accessing the resource (Equation 3).

\subsection{Software and Hardware}
\indent Both virtualization platforms have the same software stack installed (Figure \ref{fig:softStack}).  
We use the Xen hypervisor and CentOS release for both the guest (DomU) and VMM (Dom0).  
This software stack can be extended to mulptiple servers to produce a larger cluster of servers.  
We chose Postgres as the database server for our tests because of its robustness and standard use in hosting facilities and applications.  
It is a good general purpose application that can be used to stress I/O, memory, and CPU resources, although we focused our test suite to measure database read operations.

\indent The tests can be run in all sizes of virtualization environments (Figure \ref{fig:virtSize}).  
Tests which run in a \emph{Small} or \emph{Medium} environment can also be run in a \emph{Cluster} or \emph{Cloud} based system by increasing the size of the database or number of guest machines until a similar load is reached.  
For our experiments we run on a \emph{Small} and \emph{Medium} sized virtualization environments. 

\begin{table}[h]
\begin{subtable}[h]{0.45\textwidth}
\begin{tabular}{ l p{5cm} }
  Software & Version \\
  \hline
  Hyperviser & Xen 4.2 \\
  Domain 0 & CentOS 6.2 (Kernel 3.4) \\
  Guest Domains & CentOS 6.2 (Kernel 2.6.39) PostgreSQL 8.4 \\
  \hline
\end{tabular}
\caption{Software installed virtualization test stack}
\label{fig:softStack}
\end{subtable}
\hfill
\begin{subtable}[h]{0.45\textwidth}
\begin{tabular}{ l p{5cm} }
  Size & Specifications \\
  \hline
  Small & IBM x3650 Quad Core 2GB Ram \\
  Medium & Dell PowerEdge T410 dual quadcore Xeon processors, 8GB Ram. \\
  Cluster & Multiple small or medium servers clustered together with shared SAN data store. \\
  Cloud & Amazon Cloud or similar PAAS provider. \\
  \hline
\end{tabular}
\caption{Virtualization sizes for tests}
\label{fig:virtSize}
\end{subtable}
\caption{Software and Hardware stacks}
\end{table}

\subsection{PGbench}
\indent The test infrasture generates a consistent and reproducible system load by using a PostgreSQL database server with PGbench. 
PGbench creates a TPC-B similar style workload and calculates transactions per second (TPS), so we can track the performance of each guest system from the application layer \cite{pgTune}.  
We use PGbench to both generate a load to measure overhead and create interference.
PGbench will put a stress load on the limits of the system, it is a good tool for simulating guest machines that are all consuming as much resources as possible.  
We measure the TPS from the benchmark when run as a single virtual machine and when run with multiple machine concurrently.  We compare the degradation from the application to our proposed calculation for interference.

\subsection{Domain Communication}
One of the challenges of this test suite is the coordination of the starting and stopping of the test, collection of resource counters, and the information sharing between the guests and hypervisor.  
Since we use Xen as our virtualization platform, we chose XenBus and XenStore \cite{xenbus}. 
XenBus provides an abstraction for paravirtualized drivers to communicate between domains.  
XenStore is an information storage space shared between domains.  
These tools are often used to configure and manage domains with higher level user tools, and can be configured to provide security between domains.
In order for paravitualized guest domains to communicate with the hypervisor the XenStore tools for DomU domains had to be built as this is not currently part of the standard package of guest tools.  Other options were to use the TCP/IP stack to send and receive messages between the domains to coordinate the tests and information as well.

\subsection{I/O Workload}
For I/O intensive workloads, the application tends to perform very well when the entire \emph{working set} can fit into main memory, and the OS will cache reads from disk.
However, when the \emph{working set}  approaches (or exceeds) the size of main memory, the application tends to degrade quickly.  
Our initial experiments to find an I/O workload highlight this fact by changing the database size under load in a virtual environment.
Before running the entire test suite we need to create a test database that will exceed the \emph{working set} and increase the probability that a database read will need to fetch the data from disk storage.  With PGbench the "-i" flag \emph{scaling factor} is used to initialize a database at a specified size (prior to running the benchmark) so that our results can show changes between a memory bound system and I/O bound system.  

We created a script to find the size of database required for a force I/O.  It runs and creates a small database then runs a benchmark and collects the results of the benchmark - TPS.  The script then creates a slighly larger database and runs the benchmark again.  This process is repeated until the TPS drop significantly, and the CPU time is almost idle.  At this point we can determine an adequate database size for an I/O bound system.

\begin{figure}[!h]
  \begin{center}
  \includegraphics[width=4in]{images/SmallScale.png}
  \caption{TPS on our IBM x3650 system with 1 vCPU and 512KB vRAM. It changes from a memory bound application to an I/O bound application when the DB size approache the available RAM.}
  \label{smallIO}
  \end{center}
\end{figure}


\subsection{Test Data}
\indent We can use the TPS to show how the database performs at the application layer when external environment changes are made to the system.  Our tests will select Memory and I/O resources to monitor.  For measuring disk I/O it is a good to also collect virtual memory (in this case we are talking about OS virtual memory, not virtualization) as well as I/O data.  We selected to measure paging performance counters \ref{fig:memory} across all layers for the memory resource.  We selected disk read counters \ref{fig:io} as our data at all layers to measure interference from I/O. 
We use these performance counters to try to analyze the interference and explain the degredation from virtualization.

\begin{table}[h]
\begin{subtable}[h]{0.45\textwidth}
\begin{tabular}{ l p{5cm} }
       measurement & description \\
       \hline
       pgpgin  &  count of pages in. \\
       pgpgout  & count pages out. \\
       pgfault  & count minor (or soft) page faults. \\
       \hline
\end{tabular}
\caption{Virtual memory paging performance counters\ref{memory}}
\label{fig:memory}
\end{subtable}
\hfill
\begin{subtable}[h]{0.45\textwidth}
\begin{tabular}{ l p{5cm} }
       measurement & description \\
       \hline
       r\_sectors & The number block sectors read \\
       r\_ms & The time spent reading \\
       r\_total & Total number of reads completed. \\
       \hline
\end{tabular}
\caption{I/O read performance counters \ref{iostats}}
\label{fig:io}
\end{subtable}
\caption{Test result data collected from guest and hypervisor}
\end{table}

