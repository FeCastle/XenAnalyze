\subsection{Interference}
A number of research efforts have attempted to analyze, quantify, or predict application performance when moving from a physical system to a virtual system.   Cherkesova (et. al) show that they can measure and quantify the CPU overhead for moving from a physical to virtual machine.  Additionally, they formulate a method for “charging” a machine for excessive I/O which ends up causing CPU contention on an unrelated VM \cite{cherkasova}.  Research at VMware also show this contention, and formulate that it is due to additional Inter Processor Interrupts (IPI) \cite{ahmad}.   This contention is more prevalent to the virtual environment, because the guest may run on one CPU, and the hypervisor can run on a separate CPU. 
 
\indent Disk and network I/O is a major problem when running multiple guests with I/O intensive applications such as file servers and database servers.  Research at Georgia Tech \cite{paul} analyzes the impact of combinations of I/O, CPU, and memory intensive applications running in various combinations.  They conclude that when a File Server and DB Server are placed together they result in a 25\% - 65\% degradation.  This is in contrast to CPU application research \cite{huber1, huber2} which can be allocated per VM (CPU core pinning) and will operate with about 5\% overhead.  Furthermore, they show that CPU intensive applications can scale at a rate near optimal 1/x.   For Example, if 4 CPU intensive applications running at rate x on 4 fully loaded cores, then they will all run at about 1/4 capacity if placed on the same core.   Disk I/O and memory intensive applications do not scale in this manner, because of both visible and invisible interference \cite{tickoo}.

\indent Ultimately, the goal of this type of research is to be able to automatically know a priori how a virtual machine will run in a given environment.  Recently, researchers at Florida International attempted to predict performance in a virtual environment through several performance models \cite{kundu}.  Through on-line and off-line modeling, and training data, they are able to predict with about a 20\% error rate through various linear regression models.  They also use an artificial neural network statistical model which was able to predict at under 6\% median error rate.   In order to predict this performance, the application load rate would need to remain constant.  Research from Tiketekar (et. al) at Oak Ridge National Laboratory show that it is extremely difficult to generalize a work load \cite{tikotekar}.  They show that similar HPC application benchmarks, which are both CPU bound, can have significantly different results when paired with other applications.  They stress the fact that performance isolation is extremely difficult.

\indent Prior to Xenoprof, it was not possible to read hardware performance counters from the guest OS perspective \cite{menon, du2}.  However, Xenoprof does not analyze the results or draw any conclusions about the counters.  For example, research in virtualization performance shows that there are 'invisible' resources such as the shared LLC cache \cite{tickoo}.  They show that scheduling CPU intensive VMs on the same socket but different cores will result in significant degradation.  Conversely an I/O intensive application with a CPU intensive application on the same socket with separate cores will run with little interference.  If this I/O and CPU load runs on the same CPU the I/O intensive application will perform optimally, but will significantly degrade the CPU intensive application due to multiple interrupts to read the data from the virtual controller.

\subsection{End-To-End}
\indent There has been research to find a \emph{root cause} of application degradation which could identify the layer of the problem in traditional \cite{traeger} and high-performance \cite{knapp1} systems.  This research shows the importance of analyzing external parts of the system and their impacts on performance.  It has been shown that analysis needs to be competed from a full \emph{end-to-end} perspective of the system \cite{saltzer}.  Gupta \cite{gupta1} also urges that we that there should be transparancey each layer and each layer should be charged performance points.  The research also argues that changes in workload that the application should inform systems about the current needs.  An alternate argument could be that the system should inform the layer above about the availability of the resource.  

\indent Since the system workload and available resources will continually change, applications running in a virtual environment will need the ability to analyze performance at run time.  We need to be able to examine all layers of virtualization and share that data with the other layers.
Existing resource usage tools such as \emph{top, sar}, and \emph{vmstat} in Linux OS and \emph{perfmon} in Windows require additional information from external sources of data.  
Further research is needed so that existing tools and new tools can be developed to show the virtualization impact and resource usage of a virtualized application while running with multiple concurrent machines.

