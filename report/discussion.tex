Our research has reviewed the current work in virtualization and the trends of data centers and cloud computing to move physical servers to virtual servers.  Virtualization can reduce cost and power and improve resource utilization.  With faster computers and more cores, low use applications do not need a complete physical server.  Data centers can virtualize systems and easily deploy thousands of virtual servers across fewer physical servers.  Both VMware and Xen provide tools to manage large clusters of physical servers each running multiple guest virtual machines.

Current research in managing these types of environments show the difficulties in identifying performance due to overhead and interference from virtualization.  Most of the research is to try to find the optimal solution, given set of resources, a set of workloads, and a set of applications.  However other research showed that different workloads can have severely different results, and it is difficult to generalize a workload.  Assuming that a workload can (and will) change, we need to be able to identify at runtime why an application may be degraded.  We must also consider the fact that the workload will change not only for the guest virtual machine in question, but also for external domain workloads. 

Our research uses the end-to-end argument in research and methods of performance analysis.  We identify the layers and the resources in each layer.  We find that looking at both the hypervisor and the guest machine gives the user of the guest machine a significant advantage in determining the root cause of the problem.  Without this it would cause a significant impact on the developer or administrator, since the user of the guest would not have access to the hypervisor.  Additionally, the hypervisor does not have a view of the applications on the guest.  
Our experiments only use I/O performance and we only measure interference from read counters.  A next step is to determine if this will work for write I/O and both read and write I/O concurrently. Similarly, we should try to analyze network I/O. After that is completed we can try to determine how memory intensive workloads can measure interference, and how they effect the I/O interference.  For Xen with Linux guest machines there is already a metric \emph{steal time} that shows the CPU time used by another guest, and the current guest was waiting to use the CPU.

Another area to explore is testing this on other platforms.  All of our tests were run on Xen with a Linux guest OS.  We need to test on other hypervisors such as VMware ESX and Microsoft HyperV.  We also need to run other types of guests since the counters in each guest could be different.  We believe our method is generic and most hypervisors and guests already have the data but it may need to be collected and analyzed differently.

We also calculated the overhead of virtualization $Overhead_IO$ and stated that it was important, but did not go into details about this statistic.  We believe this is useful for tuning the hypervisor and guest.  It can also be used to compare virtualization techniques.  We also found several interesting results across multiple counters above and beyond what was in this document.  We found that by looking at the sum of the counters of all guests and comparing that to the hypervisor we can calculate $Overhead_Vall$.  Often, the difference between these two values were meaningful depending on the platform and the workload.  However, we were unable to draw any meaningful conclusions about this difference. 


\subsection{Conclusions}
We need the ability to accurately provide performance counters and statistics to the guest applications based on the current availability of the physical resources.  Our research provides a method to show the interference caused by external guest applications for I/O workloads by passing resource counters through the additional layers of virtualization.    
