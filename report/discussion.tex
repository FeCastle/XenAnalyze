
Our work has reviewd the current work in virtualization and the trends of data centers and cloud computing to move physical servers to virtual servers.  Virtualization can reduce cost and power and improve resource utilization.  With faster computers and more cores, low use applications do not need a complete physical server.  Data centers can virtualize systems and easily deploy thousands of virtual servers across a few physical servers.  Both VMware and Xen provide tools and to manage large clusters of physical servers each running multiple guest virtual machines.

Current research in managing these types of environments show the difficulties in identifying performance do to overhead and interference from virtualization.  Most of the research is to try to find the optimal solution for a set of resources and a set of applications.  However other research showed that different workloads can have severely different results.  Assuming given that a workload can (and will) change, we need to be able to identify why an application may be degraded.  We must consider the fact that the workload will change not only for the VM in question, but also external domain workloads. 

Our research uses the end-to-end argument in research and methods of performance analysis.  We identify the layers and the resources in each layer.  We find that looking at both the hypervisor and the guest machine gives the user of the guest machine a significant advantage in determining the root cause of the problem.  Without this it would cause a significant impact on the developer or administrator, since the user of the guest would not have access to the hypervisor.  Additionally the hypervisor does not have a view of the applications on the guest.  


\subsection{Future Work}
As much as this research may help with future performance analysis on virtual systems, there is still much work that needs to be completed.  We previously mentioned that that security and performance are not a major concern, and the benefits of this additional data outweigh these concerns.

The most obvious fact is that our experiments only use I/O performance and we only measure interference from read counters.  A simple next step, which would be very similar, is to determine if this will work for write I/O and both read and write I/O concurrently. Similarly, we should try to analyze network I/O. After that is completed we can try to determine how memory intensive workloads can measure interference, and how they effect the I/O interference.  For Xen with Linux guest machines there is already a metric \emph{steal time} that shows the CPU time used by other guest, and my guest was waiting.

An alternate approach would be to build some analytics or machine learning to determine how the resources are used at each layer effect each other.  By running many different workloads and collecting all the statistics at each layer we may be able to make some better decisions about how certain workloads effect each other.  Currently we only related a few of the counters, but it could be that I/O may effect CPU, interrupts, or any other statistic.  We determined the relationship though trial and error and multiple spreadsheets.  Only through a complex set of analytics could we identify relationships between workloads in guest domains causing interference.

Another area that would need more work would be testing this on other platforms.  All of our tests were run on Xen with a Linux guest OS.  We need to test on other hypervisors such as VMware ESX and Microsoft HyperV.  We also need to run other types of guests since the counters in each guest could be different.  We believe our method is generic and most hypervisors and guests already have the data but it may be collected and analyzed differently.

We also calculated the overhead of virtualization $Overhead_V$ and stated that it was important, but did not go into details about this statistic.  We believe this is useful for tuning the hypervisor and guest.  It can also be used to compare virtualization techniques.  We also found several interesting results across multiple counters above and beyond what was in this document.  We found that by looking at the sum of the counters of all guests and comparing that to the hypervisor we can calculate $Overhead_Vall$.  Often, the difference between these two values were meaningful depending on the platform and the workload.  However, we were unable to draw any meaningful conclusions about this difference. 


\subsection{Conclusions}
We need the ability to accurately provide performance counters to the applications based on the current availability of the resources.  Our research provides a method to show the interference caused by external guest applications for I/O workloads.  We did not know about the CPU \emph{steal time} when we started this work, but it has shown to be a valuable metric for applications running in the cloud.  We believe that I/O bound applications could also benefit from quickly knowing that external systems could be the cause of poor performance.
