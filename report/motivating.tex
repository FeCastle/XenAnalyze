A large corporation, with over 10,000 employees, used a suite of applications which were specifically written for this business. The software stack consisted of Redhat based Linux with 2.6 kernel with current patches, PostgreSQL 8.3.6, and Apache 2.2.3.  The physical hardware had 40 CPU cores (4 sockets 10 cores each) and 256 GB of RAM with a high performance RAID SAN used for the data store.  At most times during the business day the system would run with less than 60\% total CPU and would cache nearly all data inside active RAM.  The \emph{working set} was less than the available memory.  The system ran optimally for several months in this configuration.

\indent At some point the customer and data center decided to virtualize the application to add disaster recovery \cite{wood} and save power \cite{lim}.  After several weeks in this configuration, the end users started to complain and enter trouble tickets about slow response times.  Application developers and support quickly found issues and developed fixes.   Usually these fixes were to reduce the number or reads or writes to some file in the file system.  However, after each fix, sometimes days later, a new problem would show up.  Eventually the database engineers and system engineers began examining the recurring problems.

\indent The symptom was that sometimes the system would go into a state where all 32 virtual CPUs \footnote{The virtualization platform was limited to 32 virtual cores} would run at 100\% CPU time, multiple threads would go to a “D state”, \footnote{According to the man page for 'ps':  A process in the D state is neither running nor sleeping.  It is "Uninterruptible sleep (usually IO)"} and multiple processes would fail to complete.  The additional CPU time in the virtualized guest was shown to be in \emph{System Time} and multiple tools confirmed this.  Memory and I/O used on the guest and host server were consistent, and engineers monitoring the SAN did not see any latencies or problems measuring disk IO.  Sometimes this would only last a few seconds and sometimes it would last for hours.  Tracing system calls with \emph{strace} showed some process would wait for several seconds on \emph{open}, \emph{lseek}, and \emph{write} (Figure \ref{fig:syscall}).

\begin{figure}[h]
%\begin{Verbatim}[frame=single]
\begingroup
    \fontsize{10pt}{12pt}\selectfont
\begin{Verbatim}
12:46:02 open("…", O_WRONLY|O_CREAT|O_
12:46:16 fstat64(61, {st_mode=S_IFREG| 
12:46:16 lseek(61, 0, SEEK_CUR) = 0 
12:46:24 write(61, "O:6_"..., 8192) =  
12:46:24 write(61, "R1339 "..., 8192)  
12:46:24 write(61, "ct "..., 8192) = 8 
12:46:31 write(61, "cription "..., 819  
12:46:31 write(61, ";s:11 "..., 8192)  
12:46:31 write(61, "7:for "..., 3288)  
12:46:31 close(61) = 0
\end{Verbatim}
\endgroup
\caption{\emph{strace} shows system calls to open, lseek, and write take up to 14 seconds to complete}
\label{fig:syscall}
\end{figure}

Data center engineers examined the statistics in the host VMM, which showed exactly what was noticed on the guest:  High CPU usage.   However, from the point of view of the VMM, there was no information about the guest applications or kernel.  Furthermore, the VMM could not distinguish between guest kernel time and guest user time.  The problem could not be confirmed to be caused by virtualization, but no symptoms appeared until the application suite was virtualized.  The guest was unable to perform profiling with hardware counters on this version of the hypervisor \cite{serebrin} to see if there was a kernel bug.  Was there a bug or misconfiguration in the application, guest OS, hypervisor, or hardware?  Or was there just too much overhead from virtualization during peak usage? 

The temporary solution was to reboot the guest machine periodically, which would clear the working cache memory, and possibly have other unknown results.  After several weeks of this, enough of the load was reduced in the application to prevent this strange interference.  At the time of this writing, it is still unknown why this occurred when the system appeared to have plenty of physical and virtual resources available. There may have been interference from virtualization or a defect in the guest kernel or some combination that was not detectable. 

Similar issues have been seen at large scale streaming applications hosted in massive public clouds.  
It is argued that Netflix will completely shutdown and restart a virtual machine on a separate server if they can detect interference from external virtual machines \cite{netflix}.

\begin{comment}
They are using a new feature in the \emph{top} command called \emph{steal time} to detect if too much of the CPU is being used by other virtual machines.

\indent These examples show the need for additional tools and resource utilization techniques are needed in the guests and the VMM.  
Our experiments will reproduce similar performance problems when multiple guests are run concurrently. 
We show that even when resources are evenly distributed there is additional interference that is undetectable with current tools. 
\end{comment}
