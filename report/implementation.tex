In this section we present a framework to collect and analyze data needed for performance diagnosis from the point of view of a virtual guest system. 
First, we define the new layers of abstraction and virtual resources unique virtual environments.  
Then we identify the additional information needed from these layers, and a method to collect the data.
Finally, we describe a method to analyze the additional data to determine if the guest machine is experiencing external I/O interference.

% 1 Define the new layers of abstraction virtual environments.
\subsection{Abstraction Layers and Resources}
On a single physical server, the application, OS, and hardware layers need to be considered as a potential layer for application performance problems.  On a physical server, the resources are physical hardware such as a disk, memory (RAM), and the CPU cores.  Applications access the physical resources through the kernel, and the kernel tracks statistics about the usage of the resource.

\missingfigure{Physical Server Layers: Application, OS, Hardware}

In a virtual environment, an OS kernel in the guest virtual machine does not have direct access to the physical resources.  Instead the hypervisor divides the physical resources between the guest systems and presents a virtual resource to the guest OS.  
This is similar to an OS kernel dividing up the CPU time between multiple processes, where each process only runs for a portion of the total CPU time.  
The major difference between an OS kernel dividing up a physical resource for multiple processes and the hypervisor dividing up a resource for multiple virtual machines is that the OS kernel provides APIs and user space tools to monitor the resources.  By default (on Linux and Windows) any process can determine the percentage of resources that it is using.  Unlike the OS kernel layer, the hypervisor layer provides little or no information to the layer above it about the true availability and use of the resource.

\missingfigure{Virtual Server Layers: Application, OS, Hypervisor Hardware}

\begin{figure}[!h]
  \begin{center}
  % \includegraphics[width=6in]{images/LayersAndResources.png}
  \caption{New layer \textbf{Hypervisor} should share information with guest OS about physical resources.}
  \label{LayersAndResources}
  \end{center}
\end{figure}

The physical resource may be used by an external virtual machine or by the hypervisor, and there is little information that a guest OS or application can see about the physical resource.
Additional information is needed from hypervisor about the physical resource so that the administrator, OS, or application can make better decisions about the availability of the resource. 

\begin{table}[h]
  \begin{tabular}{ l p{10cm} }
    Resource & Definition \\
    \hline
    vCPU & The virtual core allocated to the guest \\
    vMemory & The virtual RAM allocated to the guest \\
    vDisk I/O & The virtual I/O block device allocated to the guest \\
    \hline
Â  \end{tabular}
\caption{Virtual resources which may experience interference from hypervisor or external guest.}
\label{tab:resources}
\end{table}

% 3 Identify the performance counters which can be used to measure I/O performance on a virtual guest machine.
\subsection{Performance statistics}
To monitor applications and the kernel, administration tools will read kernel statistics over some period of time.  Then the tool will aggregate and relate the data for that time period.  In some cases, additional inference can be made from different parts of the data.  For example the \emph{sar} utility can show the average wait time, average service time, and \emph{bandwidth utilization} for I/O.  On a physical server, where the hardware availability is relatively static, this information is valuable for analyzing and tuning applications.  However, when the resource is virtualized, these kernel resource statistics for virtual resources are almost meaningless.  Our tests will show that for disk I/O statistics, there is no difference between application changes and interference from external virtual guests. 

\begin{figure}[h]
\begin{algorithmic}[H]
 \STATE $interval \gets 5$
 \STATE $stat \gets$  DiskRead       
 \STATE $pre \gets $ READ $stat$ 
 \LOOP
    \STATE SLEEP $interval$
    \STATE $post \gets$ READ $stat$
    \STATE $result \gets (post - pre)/interval$
    \STATE PRINT  $result$
    \STATE $pre \gets post$ 
 \ENDLOOP
\end{algorithmic}
\caption{Example to display reads per second \emph{rd/s} every 5 seconds.}
\label{alg1}
\end{figure}

Our method will collect, aggregate, and analyze the statistics kept by the OS kernel in each guest domain and the hypervisor in order to measure interference from external domains and the hypervisor.  We find that there are kernel statistics that will show the interference from external machines, when collected from all layers.  For our examples we choose disk read and virtual memory statistics to analyze I/O bottlenecks.  However, this method could be used for identifying other types of performance problems if the data was available to the guest.

\begin{table}[h]
\begin{subtable}[h]{0.45\textwidth}
\caption{Virtual memory paging performance counters \cite{memory}}
\begin{tabular}{ l l }
       pgpgin  &  count of memory pages in. \\
       pgpgout  & count of memory pages out. \\
       pgfault  & count minor (or soft) page faults. \\
\end{tabular}
\label{fig:memory}
\end{subtable}
\hfill
\begin{subtable}[h]{0.45\textwidth}
\caption{I/O read performance counters \cite{iostats}}
\begin{tabular}{ l l }
       r\_sectors & number of sectors read successfully. \\
       r\_ms & number of milliseconds spent by all reads. \\
       r\_total & number of reads completed successfully. \\
\end{tabular}
\label{fig:io}
\end{subtable}
\caption{Statistics collected from all guests and hypervisor}
\end{table}

By collecting these statistics at two points in time as in algorithm \ref{alg1}, we can infer other data such as the throughput in reads per second \emph{r/s}.  We can also calculate the disk I/O latency or the average wait time for reads \emph{AvgRdWait}.  
Since \emph{r\_ms} is the total number of milliseconds spent by all reads, we can divide this by the total number or read requests to get the average wait time.  This is very important since external guests may cause I/O delays in a shared disk.
\newline\newline
\Large
	$AvgRdWait = \frac{r\_ms}{r\_total}$ \\
	$read/s = \frac{r\_total}{TotalTime}$ \\
\normalsize

% 4 A method to calculate the overhead and theoritical maximum performance using an offline modeling technique.
\subsection{I/O Virtualization Overhead}
For each virtual resource (Table \ref{tab:resources}) there is a performance cost to making that resource virtualized.  If the guest had direct access to the hardware, at all times, the virtualization cost would be zero.  Since most system calls from the guest kernel need to go through the hypervisor, we need to account for this additional time.

Several researchers \cite{cherkasova, huber1} have called this cost the \emph{overhead}, and have quantified the overhead for a given configuration.  This previous research used an offline modeling technique by running a benchmark with and without virtualization.  By calculating the percent difference of these two benchmarks they can calculate the overhead.
The problem with this technique is that physical servers would need to be provisioned for this exercise.  Any configuration changes in hardware or anywhere in the software stack, may require a new test.  

Our method uses a similar approach, but does not require physcial harware for each guest.  We calculate the percent difference between the physical resources hypervisor and the virtual resources guest. 
We assert that the time spent waiting for an operation to complete in one layer depends on the layer below.  
In a physical server the application depends on the kernel and the kernel depends on the hardware.  
Our overhead calculation finds this additional time for I/O virtualization, by using the $AvgRdWait$ statistic in both the guest and hypervisor without interference to calculate the overhead.

We need to calculate the \emph{overhead} before running a guest virtual machine in production. 
In large scale datacenters and cloud systems, templates are usually created before virtual machines are used in production.  A template is a complete snapshot image of a virtual machine that has been built and tested to meet some need.  For example a Redhat 6.2 system with an Apache web server may need to be used on several machines.  A system could be built, tuned, and tested for that enviroment and then made into a template.  Future users can deploy a new virtual machine from that template.  We are suggesting to calculate the overhead from virtualization when it is made into a template.  

To calculate the virtualization overhead, create a single virtual machine with dedicated resources on isolated hardware.  
There are many performance benchmarks that can be used \cite{katcher, tikotekar, hplBench} to place a desired load on the virtual machine. 
Then run the benchmark or load on the virtual machine and begin monitoring the performance statistics for that resource using the algorithm in Figure \ref{alg1} in BOTH the guest OS and hypervisor. 
Save the results of the guest, hypervisor, and time in the guest as this is the baseline statistic without external interference.
It is important that both the guest and hypervisor collect PRE and POST statistics as close to the same time as possible.  

Since we are measuring from the perspective of the guest OS, we want to discover how the guest degrades in comparison to the hypervisor.  
For some statistics such as the \emph{AvgRdWait}, we can approximate the overhead of virtualization, by calculating the average wait time in the hypervisor \emph{AvgRdWait\textsubscript{H}} and the guest \emph{AvgRdWait\textsubscript{G}}.

\begin{equation}
  Overhead_{IO} = \frac{AgvRdWait_G - AvgRdWait_H}{AvgRdWait_H} 
\end{equation}

% Simple method %
\subsection{I/O Virtualization Interference}
We should be able to distinguish between a guest that is degraded because the hypervisor layer (including external interference) is using the resource and a guest that is degraded because of an application or OS layer issue within the guest.  In other words we should be able to distinguish between a problem inside the guest and a problem where the guest has no control.  An secondary goal is to measure or analyze the interference.  We should be able to answer the question, "How much of the resource is available to the guest?".

It may seem possible to that we can deteremine degredation by examining only the resource statistics inside of the guest machine.  However, with some simple tests, we were able to change the I/O latency and throughput in a guest domain by only changing the guest workload and data (Table \ref{tab:guestOnly})
 
\begin{table}[h]
  \begin{tabular}{ l l p{10cm} }
    Test & $AvgRdWait$ & $reads/ms$  \\
    \hline
    Baseline                     & 5.95 & 406 \\
    Increase DB size             & 8.04 & 563 \\
    Increase DB Connections      & 20.3 & 875 \\
    Baseline with Interference   & 9.86 & 242 \\
    \hline
Â  \end{tabular}
\caption{Only calculate based on guest view of virtual resources.  The first 3 tests are without interference from external domains.}
\label{tab:guestOnly}
\end{table}

When an application is running in the guest domain the availablity of the resource may not be available to the guest at some time.  From this information it is difficult to determine whether the increase is due to external interference or application changes. We could also decrease the $reads/ms$ simply by reducing the I/O load.  In order to quickly determine if the guest domain is experiencing interference we need to also examine the counters in the hypervisor.  We can examine the throughput of the guest $read/s_G$ and the throughput of the hypervisor $read/s_H$ over the same time period calculate the interference as:
\begin{equation}
	Interference_{RPS} = \frac{read/s_H - read/s_G}{read/s_H} 
\end{equation}

% See Test7_1Results.xls - Barbaro Exp7.2 and Barbaro_Dom29.txt
\begin{table}[h]
  \begin{tabular}{ l l l p{6cm} }
    Test & Guest & Hypervisor  & $Interference_{RPS}$ \\
    \hline
    Baseline                     & 406 & 372 & -6\% \\
    Increase DB size             & 563 & 538 & -6\%  \\
    Decrease DB Connections      & 875 & 836 & -5\%  \\
    Baseline with Interference   & 242 & 691 & 60\%  \\
    \hline
Â  \end{tabular}
\caption{Reads per ms of Virtual Guest machine and hypervisor.}
\label{tab:HypervisorGuest}
\end{table}
By looking at the throughput from the hypervisor view, we can quickly determine if a guest application is degraded due to external interference from external domains.  

There is a case when this could erroneously tell us there is interference when there is not any interference.  So far we have only examined systems where the application is running at 100\% of possible throughput.  One reason virtualization is so successful is that not all guests run at full capacity at all times.  In a perfect virtualization a physical resource $R$ would be divided between $n$ guests, and each guest would use $1/n$ of resource $R$ at any given time.  For example, if a physical disk could handle a throughput rate of 400 reads per second, and 4 virtual machines each requested data at a rate of 100 reads per second, then the previous method would report 75\% interference.  

Since the guest has previously calculated the latency through the hypervisor $AvgRdWait$ when we calculated $Overhead_V$, we can use that information to determine if the guest is degraded due to interference from external systems.  When external systems are exceeding the I/O throughput, the $AvgRdWait$ time will increase significantly.  We can calculate the interference by calculating the controlled average wait time (without interference) and the average wait time when run with additional external domains.  Since we are trying to determine if the hypervisor is the root cause, we need to use the information from the view of the hypervisor.
\begin{equation}
	Interference_{ARW} = \frac{AvgRdWait_{Current} - AvgRdWait_{Overhead}}{AvgRdWait_{Current}} 
\end{equation}

\subsubsection{IO Example Method}
First, the guest machine needs to calculate the $Overhead_V$ and save the $AvgRdWait_H$ for later reference.  Then when the guest machine is put into production, it can calculate the interference when requested from a user space tool.  Both the guest virtual machine and hypervisor can calculate the resource statistics according to algorithm in \ref{alg1} when requested.  It is important that they collect $PRE$ and $POST$ statistics concurrently.  Then the hypervisor can return the $AvgRdWait$ and the $reads/s$ back to the guest.   The guest VM can calculate $Interference_{RPS}$ and $Interference_{ARW}$ as described previously.   Then the guest can report external interference as follows.

\begin{figure}[h]
\begin{algorithmic}[H]
 \STATE $Interference_{EXT} \gets Interference_{RPS} X Interference_{ARW}$  
 \IF{$Interference_{EXT} < 0$ } 
 	\STATE $Interference_{EXT} \gets 0$
 \ENDIF
\end{algorithmic}
\label{alg2}
\caption{User tool for guest domain to show read I/O interference.}
\end{figure}

\indent As an example: the userspace tool \emph{iostat} reads disk performance counters in /proc/diskstats and will report transfers, bytes read, and bytes written per second.  If an application was experiencing I/O performance problems this would be a tool an administrator or application developer may monitor.  Without knowing the interference this could be misleading as to the root cause of the problem.  The following example shows a possible output from the perspective of the running guest when experiencing I/O interference from external guest machines.

\begin{figure}[h]
\begin{Verbatim}
Device:  tps    kB_read/s    kB_wrtn/s
sda   577.20     41388.00    148073.00
 virtual I/O interference 22.4%     
\end{Verbatim}
\label{fig:iostat}
\caption{Example:  \emph{iostat} with additional calculated interference.}
\end{figure}

\subsection{Other Considerations}
There are three other issues that need to be considered with this type of method.  First, we examine the implications of mulitple different types of virtualization platforms and guests running.  Second, we see if there are any performance issues with the method we describe.  Finally, we look at security concerns as passing information about external machines may reduce the security guarantees when a system is virtualized.

Most virtualization platforms have a message passing API native to the system.  Usually this is to automatically configure the guest domain when it is provisioned or when changes are needed.  For Xen Server, this is done through the Xenbus and Xenstore.  VMware guest tools has a guest API (vmware-tools-guestlib package), and even has a setting to pass Host resource counter data to the guest.  However, this is disabled by default and security tuning documents encourage explicitly setting it off for security reasons \cite{vmwarepubs}.  If there were not any native mechanism to pass data between the guest and the host system, we could still use some message passing over TCP/IP such as SOAP or Rest service wiht HTTP.
%https://www.vmware.com/support/developer/guest-sdk/guest_sdk_40.pdf

There is some performance overhead with this additional data collection, as with any method that attempts to measure performance.  We do not want a tool to negatively impact the performance, or report degredation from the tool itself.  For I/O performance issues we believe that our method will not have a measurable impact.  For a system that is waiting on I/O from disk, it should have CPU cycles available for data processing.   Both the guest and the hypervisor need to read in memory kernel data twice.  Then each needs to compute the difference of the two statistics after they are collected.  The hypervisor needs to pass the data to the guest VM, and then the guest needs to do a few calculations on the data.  There are several tools in Windows and Linux that perform similar to this now, and are widely accepted.

Obviously security is a major concern for any system.  The more information an attacker can gain about the system, the better chance that a vulnerability can be found.  There is similar precendence for sending this type of data in a standard kernel.  Currently (by default) all users can see the sum of all the I/O and cpu processes.  This is the same type of data where we want to know how our virtual machine is effected by other virtual machines.  An attacker may be able to gain knowledge about the run state of external machines, and possibly execute some side channel attacks.  More research would be required to see what types of exploits would be possible.  

