In the following test experiments we evaluate the benefits of passing additional information through the layers of the hypervisor and guests for profiling.  Unless otherwise noted, each experiment is performed on a \emph{Personal} and \emph{Business} scale virtualization platform (Table \ref{virtSize}).  Both systems have CentOS and Xen Server installed (Table \ref{softStack}) for the software stack installed.

\subsection{External machine interference}
In this experiment, we show the memory and I/O interference from external systems.  From the guest domain running the benchmark, there is a significant drop in the application's performance when run concurrently with external systems.  Additionally, monitoring resources in the guest domain provides little information about the performance problems. 

First we divide the physical memory and CPUs into four equal parts and create four individual guest virtual machines (Dom1 - Dom4).  Each virtual machine is given 25\% of the available resources so that no guest virtual machine would interfere with another machine if they were on separate physical systems.  We start only our Dom1 machine and run our experimental test with PGbench.  On our \emph{Business} Dell server, Dom1 is allocated 2GB vRAM and 2 vCPU, while on the IBM server Dom1 is given 512MB vRAM and 1 vCPU.  

\subsubsection{Database Size}
We start by initializing a Small DB and run PGBench and collect the TPS.  Then we increase the DB size slightly and run the benchmark again.  After repeating this process several times, we can see that when the DB size changes to an I/O bound system its performance drops significantly (Table \ref{fig:tps1}) .  This is due to the fact that it must fetch database rows from the disk and it is much slower.  The Dom1 on both servers begin to degrade significantly to I/O when the DB size is close to the available vRAM.

\begin{description}
  \item[Small DB] The working set can fit into main memory.  Performance is based on Memory.
  \item[Medium DB] The working set is swapped to disk occasionally. Performance is moving from Memory to I/O.
  \item[Large DB] Most reads need to go to disk.  Performance is based on Disk I/O.
\end{description}

% Section 7.1.2
\subsubsection{With Interference}
Then we simulate external machine interference by running Dom1 concurrently with Dom2, Dom3, and Dom4. Each of the Dom2 - Dom4 systems are configured the same as Dom1.  We create a 2 GB database on each guest of the Dell, and a 1GB database on each guest of the IBM.  We run a PGBench in a loop to continuously create I/O interference on each external guest machine (Dom2 - Dom4) while going from a small DB to a large DB on Dom1.

We again measure the results of the benchmark on Dom1, while the other 3 guest domains are causing interference.  When the system is not I/O bound (Small database) there is about a 28\% drop in performance (4434 TPS - 3208 TPS) on the IBM server and little change in the Dell server (Table \ref{fig:tps1}).  We can see that on both servers it drops to an I/O bound system much quicker.  And there is significant interference from the I/O on both servers for a large database.

\begin{table}[h]
\begin{subtable}[h]{0.45\textwidth}
  \begin{tabular}{ l | r | r | r }
    DB Size & Single & Interfernce & Drop \\
    \hline
    Small & 4434 & 3208 & 28\% \\ \hline
    Medium & 2149 & 216 & 90\% \\ \hline
    Large & 260 & 197 & 24\% \\  \hline
    \hline
  \end{tabular}
\caption{IBM x3650 with 2GB RAM:  Each Guest domain has 512MB Allocated.}
\label{fig:tps1}
\end{subtable}
\hfill
\begin{subtable}[h]{0.45\textwidth}
  \begin{tabular}{ l | r | r | r }
    DB Size & Single & Interference & Drop \\
    \hline
    Small & 5772 & 5734 & 0.7\% \\ \hline
    Medium & 1608 & 162 & 90\% \\ \hline
    Large & 359 & 82 & 77\% \\  \hline
    \hline
  \end{tabular}
\caption{Dell T410 with 8GB RAM:  Each Guest domain has 2GB Allocated. }
\label{fig:tps2}
\end{subtable}
\caption{Dom1 TPS (Transactions Per Second) for 3 database sizes, when run as a single VM and with 3 external guests running concurrently.}
\end{table}

% Section 7.1.3
\subsubsection{Existing tools analysis}
\indent Trying to analyze the performance drop in Dom1 without knowing about this external interference is difficult.  
There were no changes to Dom1 when run by itself and run with other guests.  
By using the system \emph{sar} utility we can examine available memory, SwapIn and BytesIn/s to see if we can determine why the application benchmark is degraded without collecting external information (Figure \ref{fig:vmstat}).  
Memory is not used as efficiently, the system almost completely elimiates swapping data in, and also can't read as quickly from disk.  
However, there is no indication that the problem was due to an external guest and hypervisor using those resources.
A DBA looking at these numbers may conclude that kernel swap or DB tuning may fix the problem.  
The root cause of the performance drop is due to external interference, which is not known from examining the data available.

\begin{table}[h]
  \begin{tabular}{ l | r | r | r }
    VMstat & Single & Multiple & Drop \\ \hline
	Memory & 211Kb & 138Kb & 35\% \\
	SwapIn/s & 1,480 & 85 & 94\% \\
	BytesIn/s & 6,877 & 4,438 & 35\% \\
  \end{tabular}
\caption{Some resource statistics collected with vmstat on Dom1 on a Large database while running alone and with multiple systems.} 
\label{fig:vmstat}
\end{table}

% Section 7.2
\subsection{Personal Server}
In the previous experiment we showed the interference that can occur when exteranl I/O interference is applied to a guest domain.
In this experiment, we create large I/O bound database, which is greater than the size of virtual RAM on the Dom1 test machine.  
The results are from our IBM x3650, where each guest is configured to use 512MB of vRAM.
First, we run the benchmark and measure the overhead on a \textbf{Large DB} at 640MB in Dom1 using the method in Section 5.  
Then we start Dom2 - Dom4 with two different database sizes in the guest machines and measure the interference these servers cause on an I/O bound system.  We compare the results of the calculated interference with the performance drop in the application.

\begin{Verbatim}
# xm list
Name                                        ID   Mem VCPUs      State   Time(s)
Domain-0                                     0  1988     4     r-----  88871.2
Test_VM1                                     1   512     1     -b----  60187.0
Test_VM2                                     2   512     1     -b----   5722.0
Test_VM3                                     3   512     1     -b----   5273.0
Test_VM4                                     4   512     1     -b----   5203.5
\end{Verbatim}

% Section 7.2.1
\subsubsection{Overhead}
We calculate the overhead for the guest OS paging and the I/O reads using the resource counters defined in our test suite.  While calculating the overhead we also collected the application performance at 415 TPS without any external interfernece.

% See Test7_1Results.xls (Barbaro - Exp7.2(P2))
\begin{table}[h]
\begin{tabular}{ l l l p{5cm} }
  Counter & Dom0 & Dom1 & $Overhead_V$ \\
  \hline
	pgpgin    & 332,504 & 165,901 & 100\% \\
	pgfault   &  25,648 & 132,691 & -81\% \\
	r sectors & 332,504 & 331,803 &   0\% \\
	r\_ms     &  63,356 &  72,428 & -13\% \\
	r total   &  11,145 &  12,166 &  -8\% \\
  \hline
\end{tabular}
\caption{I/O Overhead calculated average of four test runs on the IBM x3650.}
\label{fig:OverheadSmall}
\end{table}

The first thing to notice on this is that the pages in and the read sectors are exactly  the same in the hypervisor, but in the guest domain the read sectors are about twice as much as the pages in.  The other counters all show a negative overhead because the hypervisor performs less physical requests to disk and page faults than were completed in the guest.  For example the guest issued about 12 thousand reads, but the hypervior only actually executed 11 thousand total read requests.  This is likely due to optimizations in the hypervisor to cache data.

% Section 7.2.2
\subsubsection{Memory Interference}
We repeat the previous experiment with 4 guest domains all running at the same time.
Since the DB size in Dom1 is 640MB and there is 512MB of vRAM on Dom1, we can say that Dom1 is bound by disk I/O speed.
We run an experiments with the other 3 guest domains running a memory bound database.  Dom2 - Dom4 still has 512MB vRAM.  
We create a Small DB of 128MB in each of the other domains to generate memory interference on Dom1.
By using Equation 2 and 3 we can calculate the $Overhead_Vall$ and the $Interference$ for each counter.

\begin{table}[h]
\begin{tabular}{ l l l l p{5cm} }
  Counter & Dom0 & $\sum{DomU}$ & $Overhead_Vall$ & $Interference$ \\
  \hline
	pgpgin    & 328,144 & 163,784 & 100\% &   0\% \\
	pgfault   &  57,293 & 481,541 & -88\% &  -7\% \\
	r sectors & 328,144 & 256,352 &  28\% &  28\% \\
	r ms      & 103,483 &  70,751 &  46\% &  59\% \\
	r total   &  13,365 &   9,547 &  40\% &  48\% \\
  \hline
\end{tabular}
\caption{Interference calculated from Small 128MB DB in Dom2 - Dom4.  TPS dropped by 20\%} 
\label{fig:InterferenceSm}
\end{table}

From these tests, Dom1 had 331 TPS, and was degraded from 415 TPS.  We see significant interference in the read counters.  It is not shown in these results, but Dom2 - Dom4 was able to cache the entire working set in memory and did not issue any read requests.  Dom1 was the only domain to issue read request. The totals from all guests domains for each of the three read counters is exactly the same as Dom1.  We can see that the hypervisor read statistics were significantly greater than when we calcualted the overhead.  We also notice that there are less page faults in the hypervisor than there was when collecting overhead.

% Section 7.2.3
\subsubsection{I/O Interference}
Now we create I/O Interference in the three guest domains by creating a 640MB Large DB in Dom2 - Dom4.  Since each guest has 512MB vRAM, this should cause significant I/O contention in Dom1.

\begin{table}[h]
\begin{tabular}{ l l l l p{5cm} }
  Counter & Dom0 & $\sum{DomU}$ & $Overhead_Vall$ & $Interference$ \\
  \hline
	pgpgin    & 549,419 & 274,533 & 100\% &   0\% \\
	pgfault   &  58,201 & 255,232 & -77\% &   3\% \\
	r sectors & 549,419 & 378,075 &  46\% &  46\% \\
	r ms      & 285,334 & 184,588 &  55\% &  67\% \\
	r total   &  20,723 &  14,169 &  47\% &  55\% \\
  \hline
\end{tabular}
\caption{Interference calculated Large 640MB DB in Dom2 - Dom4.  TPS dropped by 39\%}
\label{fig:InterferenceLg}
\end{table}

During these tests, the application benchmark Dom1 showed 254 TPS.  This is much less than when run without interference (415 TPS) and with memory interference (331TPS).  All of the read counters showed significant interference.  Additionally we are showing some interference from page faults.  The overhead is still negative for all of the page faults compared to the page faults in Dom0.  But it is less than it was when originally collecting the overhead.


% Section 7.3
\subsection{Business Server}
\indent For this experiment we use our Dell server with 12GB of physical RAM. We configure all Dom1 to use 2GB of vRAM and keep Dom2 - Dom4 at 1GB of vRAM. We create an I/O bound application by creating a 3 GB database in each domain. For this size of database expect to see a significant change in the application
when the external guest domains (Dom2 - Dom4) compete for physical resources with Dom1.

\begin{Verbatim}
Name                 ID   Mem VCPUs      State   Time(s)
Domain-0              0  2048    16     r-----  22461.6
TestVM1               1  2048     1     -b----   1100.6
TestVM2               2  1024     2     -b----   1095.0
TestVM3               3  1024     1     -b----   1037.5
TestVM4               4  1024     2     -b----   1170.8
\end{Verbatim}

We calculate the overhead in Dom1 using the same method as described previously. We create a 2GB database to force I/O operations.  

\begin{table}[h]
\begin{tabular}{ l r r r r r p{5cm} }
	         & pgpgin & pgfault & r sectors & r ms	& r total & TPS \\
	\hline
	Overhead & 100\%  & -61\%	 &      0\%	 & -10\% &    -8\% &  327 \\
\end{tabular}
\caption{Average Overhead in Dom1 with 2GB vRAM}
\end{table}

This time we run Dom2 - Dom4 individually so that we can determine if each individual domain contributes to the interference.  So we run 3 separate tests in pairs with each Domain.  We calculate the Overhead and interference in these configurations.

\begin{table}[h]
\begin{tabular}{ l r r r r r p{5cm} }
	Domain  & pgpgin & pgfault & r sectors & r ms	& r total & TPS \\
	\hline
	Dom2    &	0\%	 & -3\%    & 0\%    &-1\%    & 1\%    & 152  \\
    Dom3    &  	0\%	 & -9\%	   & 0\%	& 0\%	 & 0\%	  & 158  \\
    Dom4 	&   0\%  & -8\%    & 0\%	& 1\%	 & 2\%    & 161  \\
\end{tabular}
\caption{Interference and application performance Dom1 when only 1 external guest domain was running concurrently.}
\end{table}

According to these results this did not generate the same type of performance problems as when run in the previous sets of experiments.  There was a significant drop in the application TPS, but these performance counters did not show why there was a significant application performance degradation.  We decided to run all 4 guest domains concurrently to see if we could get the same type of results.

\begin{table}[h]
\begin{tabular}{ l r r r r r p{5cm} }
	Domain  & pgpgin & pgfault & r sectors & r ms	& r total & TPS \\
	\hline
	Dom-0	& 174560 &	55192 &	174560 &	325579 &	5560 & N/A \\
    Dom-U	& 87280	 & 213934 & 174560 &    374942 &    5951 & N/A \\
	Overhead& 100\%  &   -74\%&  0\%   &    -13\% &    -6\%  & N/A \\
    Interference & 0\% & -13\% & 0\%   &     -3\% &     1\%  & 71 \\
\end{tabular}
\caption{Overhead and Interference with all 4 guest domains running concurrently.}
\end{table}

Again there is a significant performance drop from the application in Dom1, but the method did not show the interference using these counters.  There could be other counters that would show this interference?   

We calculated the reads per second during the calculation of the overhead and the calculation of the Interference by using the total reads divided by the time spent reading.  We can see a performance drop in the three different experments, but they are nearly identical between the guests and the hypervisor.  

\begin{table}[!h]
\begin{tabular}{ l r r p{5cm} }
	Experiment     & Dom0 & DomU  & TPS(Dom1) \\
	\hline
    Dom1 (Only)    & 67 & 65 & 327 \\
    Dom1 + 1 guest & 32 & 33 & 152 \\
    Dom1 + 3 guests& 16 & 17 &  71 \\
\end{tabular}
\caption{Reads/second from the view of the hypervisor and guests}
\end{table}

From this we can see that the hypervisor is degraded, and the application does not read as quickly.  There is interference from the external guest machines, but it does not appear to be from I/O type problems as in the previous experiment on the smaller size server. The information obtained here is not enough to distinguish between a slow running guest application and external interference.


% Section 7.4
\subsection{Verification without interference}
In the previous two experiments, we showed how our method can give a guest domain vital information when it is degraded from external interference.  However, what if the guest domain was degraded due to an application bug, DB change, or misconfiguration?   In that case we want our method to show that there is not any external interference.

\indent As shown previously, we use the fact that an application will degrade significantly from a memory bound system to an I/O bound system.  This is a real problem as DB can grow and needs to be purged or partitioned.  From our previous experiment with the Dell and Dom1 using 3GB vRAM.  We can use the overhead previously calculated and run 2 different tests.  The first test will use a 2GB database which will be bound by memory and will have a high TPS. The second test will have a 3GB database and will have have a much lower TPS.  Dom2 - Dom4 will be running but will not have any load or benchmark running (basically idle).








\begin{comment}
\begin{table}[h]
\begin{tabular}{ l l p{5cm} }
  Statistics & Description \\
  \hline
  /proc/diskstat & I/O statistics of block devices \\
  /proc/vmstat & Detailed virtual memory statistics\\
  \hline
\end{tabular}
\caption{Disk I/O performance counters on Linux.}
\label{tab:iocounters}
\end{table}

\subsection{}
Here we repeat the previous experiment with a large database, which is larger than available memory. We use the framework described in section 5.5 to collect I/O and virtual memory data from the /proc/diskstat and /proc/vmstat kernel statistics from all domains and the hypervisor.  From this data we can determine the impact of external I/O interference on our guest domain.\\
Before running the experiment, we need to calculate the I/O overhead by running the "dd" utility in a guest domain and collecting data without any interference.  After 3 runs we can see that the 


In this experiment we are a small virtual system: an IBM x3650 single socket quad core Intel Xeon CPU.   This machine runs CentOS 6.4 with a Xen configured kernel at 3.4.54 for Domain 0, and the Xen 4.2 hypervisor complied and packaged as part of the CentOS add on packages.   In this case Only the Dom0 is used and 3 separate memory configurations are used to show the difference between a system configured with 512MB, 1024MB, and 2048MB of ram.

\subsection{External Guest Interference}
In this experiment there we show the interference caused by running multiple virtual machines at the same time.  The following experiments are run with 4 CPUs and 2GB ruam divided between 4 virtual machines.  Each machine has 512MB and a single CPU.   In the Independent test all 4 tests are run at different times, while the concurrent test all 4 machines are run at the same time.  The reslts of the Physical machine are shown to show the Possible Maximum. 
Figure for the 512MB TPS is reported as the SUM of 4 machines.

\subsection{Guest Application Problem}
To similate a problem with a guest application we use use a misconfigured postgres database by changing the default blocksize to misalign with the guest OS ( and physical) page size.  
\textbf{--with-blocksize=BLOCKSIZE}
Set the block size, in kilobytes. This is the unit of storage and I/O within tables. The default, 8 kilobytes, is suitable for most situations; but other values may be useful in special cases. The value must be a power of 2 between 1 and 32 (kilobytes). Note that changing this value requires an initdb.  This will cause excessive reads and writes [40].  By default, the PG Block size is 8K and most (Linux) OS are configured to use 4k or 8k pages.   We use PGbench to generate an IO based workload which shows a performance degradation.  When the analyzer is run it will show a problem with the Guest Application.

\subsection{Cache Contention Problem}
Running multiple IO and memory intensive guests on the same core can result in LLC cache contention causing a performance degredation below the SLA.  PGbench can be run in a “select only” mode, in which most of the DB relations will be cached in memory.  If another DB guest is run on the same CPU socket the memory intesive application will interfere with the DB server [10].   We schedule two guest machines to run on CPU Socket 0 (core 0 and 1).  The analyzer detects the LLC cache interference and the DB server is significantly degraded below the SLA.  When moved onto separate sockets (socket 0 and 1) both the DB server and “Select Only” DB server run within the SLA.
4.6  I/O Contention 
In many cases multiple database servers could be shared on a system with multiple guests if each database only reaches peak throughput seldomly.  In this experiment we run 1 DB server per core and run the PGbench benchmark on each guest simultaneously.  If a single DB server can perform at x TPS, then it may be expected that 4 DB servers can operate at peak capacity at x/4 TPS.   Additionally, we can use PGreplay to see if we can run each system at a fraction of its speed (in this case 25\%) and achieve the same throughput on 4 separate guests.  Since there is additional overhead with the virtualiztion in DOM 0, there will be some overhead associated with the IO, and this will not be achievable.   Since a DB server synchronizes and journals the writes, each guest server will need to wait for the disk writes to complete.  The analyzer will detect the IO bottleneck issue and report problems.

\subsection{I/O Contention with ‘Disk Pinning’}
In virtualiztion it is possible to “Pin” a guest OS to a single core.  This is a way to make certain that each guest OS interference is known.  This may not be ideal in all cases, since the hypervisor may be able to better schedule guest machines on various cores (or give more cores to guests that need them).    An idea of disk pinning is similar and may (partially) alleviate some overhead in the previous experiment.  In this case, each physical disk is dedicated to a particular virtual machine, instead of a volume or raid group configured by the hypervisor.   Theoritcally, repeating the above experiment could come close to x/4 TPS except for memory cache contention.  Additionally each guest could possibly replay at 25\% of the optimal throughput on a system with 4 cores.    The issue would not be with the phyical disk I/O but with the overhead of Dom0 to schedule the disk writes.

\subsection{Virtual Cluster}
In this experiment we use a cluster of more than 1 server with a shared storage SAN server.  Each node needs an HBA with fiber connected to a fiber switch.   By running multiple guests with PG bench on the cluster server simultaneously, we can simulate an I/O contention with the shared backing store.   In a virtual cluster environment the analyzer should be able to determine that one or more guests on a particular server is exceeding their expected SLA.  Disk Pinning can also be analyzed with a large distributed backing store with multiple Logical disks (LUNs) connected with HBAs and virtualized to the guest systems.
\end{comment}

% This was the original 7.3
\begin{comment}
\begin{Verbatim}
    sync
    echo 3 > /proc/sys/vm/drop_caches
    echo 0 > /proc/sys/vm/drop_caches
\end{Verbatim}

\begin{figure}[h]
\begin{Verbatim}
# xm list
Name                             ID   Mem VCPUs      State   Time(s)
Domain-0                          0   868    16     r-----  15615.0
TestVM1                           1   4096     2     -b----    129.0
TestVM2                           2   2048     2     -b----    106.9
TestVM3                           3   2048     2     -b----     83.7
TestVM4                           4   2048     2     -b----     91.8
\end{Verbatim}
\end{figure}

\subsubsection{Overhead}
For the overhead we use Equation 1 defined in the design to calculate the OverheadV for the I/O and virtual
memory counters. We monitor the counters in the guest machines and the hypervisor. Only TestVM1 runs
while calculating the overhead.

\begin{table}[h]
\begin{tabular}{ l l l p{5cm} }
  Counter & Dom0 & Dom1 & $Overhead_V$ \\
  \hline
pgpgin    & 1,704,366 & 851,643 & -100\% \\
pgpgout   & 4,925     & 1,751   & -181\% \\
pgfault   & 50,991    & 93,843  & 46\%   \\
r sectors & 1,704,425 & 1,703,286 & 0\%  \\
\textbf{r\_ms} & 110,385 & 144,668 & \textbf{24\%} \\
r total   & 41,182 & 48,988 & 16\% \\
  \hline
\end{tabular}
\caption{Overhead calculated average of four test runs.}
\label{fig:OverheadMed}
\end{table}

\subsubsection{Interference}
Now we repeat the experiment and collect the same counters in all guests and the hypervisor, while all guests are running. 
We collect the counters in the guest and the hypervisor for 30 seconds and stop and start the collection of counters between all guests and the hypervisor. 
We calculate the overhead from all guests $Overhead_V$ (Equation 2) by calculating the sum of the counters from all guests and comparing that to the counters collected from the hypervisor. 
Then we can calculate the $Interference$ (Equation 3) in the guest domain Dom1.

\begin{table}[h]
\begin{tabular}{ l l l l p{5cm} }
  Counter & Dom0 & $\sum{DomU}$ & $Overhead_V$ & $Interference$ \\
  \hline
pgpgin    & 6,411,078 & 3,201,715 & -100\% & 0\% \\
pgpgout   & 18,541     & 8,388   & -121\%  & 60\%\\
pgfault   & 386,319    & 407,700  & 46\%   & -40\% \\
r sectors & 1,704,425 & 1,703,286 & 0\%    & 0\%\\
\textbf{r\_ms} & 1,190,041 & 1,649,907 & \textbf{28\%} & \textbf{4\%} \\
r total   & 149,994 & 181,147 & 17\%  & 1\% \\
  \hline
\end{tabular}
\caption{Interference calculated average of four test runs.}
\label{fig:InterferenceMed}
\end{table}
\end{comment}
