In the following test experiments we evaluate the benefits of passing additional information through the layers of the hypervisor and guests for profiling.  Unless otherwise noted, each experiment is performed on a \emph{Personal} and \emph{Business} scale virtualization platform (Table \ref{virtSize}).  Both systems have CentOS and Xen Server installed (Table \ref{softStack}) for the software stack installed.

\subsection{External machine interference}
In this experiment, we show the memory and I/O interference from external systems, without overcommitting the resources.  From the guest domain running the benchmark, there is a significant drop in the application performance when run concurrently with external systems.  Additionally, monitoring resources in the guest domain provides little information about the performance problems. 

First we divide the physical memory and CPUs into four equal parts and create four individual guest virtual machines (Dom1 - Dom4).  Each virtual machine is given 25\% of the available resources so that no guest virtual machine would interfere with another machine if they were on separate physical systems.  We start only our Dom1 machine and run our experimental test with PGbench.  On our \emph{Business} Dell server, Dom1 is allocated 2GB vRAM and 2 vCPU, while on the IBM server is Dom1 is given 512MB vRAM and 1 vCPU.  

\subsubsection{Database Size}
We start by initializing a Small DB and run PGBench and collect the TPS.  Then we increase the DB size slightly and run the benchmark again.  After repeating this process several times, we can see that when the DB size changes to an I/O bound system its performance drops significantly.  This is due to the fact that it must fetch database rows from the disk and it is much slower.  The Dom1 on both servers begin to degrade significantly to I/O when the DB size is close to the available vRAM.

\begin{description}
  \item[Small DB] The working set can fit into main memory.  Performance is based on Memory.
  \item[Medium DB] The working set is swapped to disk occasionally. Performance is moving from Memory to I/O.
  \item[Large DB] Most reads need to go to disk.  Performance is based on Disk I/O.
\end{description}

% Section 7.1.2
\subsubsection{With Interference}
Then we simulate external machine interference by running Dom1 concurrently with Dom2, Dom3, and Dom4. Each of the Dom2 - Dom4 systems are configured the same as Dom1.  We create a 2 GB database on each guest of the Dell, and a 1GB database on each guest of the IBM.  We run a PGBench in a loop to continuously create I/O interference on each external guest machine (Dom2 - Dom4) while going from a small DB to a large DB on Dom1.

We again measure the results of the benchmark on Dom1, while the other 3 guest domains are causing interference.  When the system is not I/O bound (Small database) there is about a 28\% drop in performance (4434 TPS - 3208 TPS) on the IBM server and little change in the Dell server (Table \ref{fig:tps1}).  We can see that on both servers it drops to an I/O bound system much quicker.  And there is significant interference from the I/O on both servers for a large database.

\begin{table}[h]
\begin{subtable}[h]{0.45\textwidth}
  \begin{tabular}{ l | r | r | r }
    DB Size & Single & Interfernce & Drop \\
    \hline
    Small & 4434 & 3208 & 28\% \\ \hline
    Medium & 2149 & 216 & 90\% \\ \hline
    Large & 260 & 197 & 24\% \\  \hline
    \hline
  \end{tabular}
\caption{IBM x3650 with 2GB RAM:  Each Guest domain has 512MB Allocated.}
\label{fig:tps1}
\end{subtable}
\hfill
\begin{subtable}[h]{0.45\textwidth}
  \begin{tabular}{ l | r | r | r }
    DB Size & Single & Interference & Drop \\
    \hline
    Small & 5772 & 5734 & 0.7\% \\ \hline
    Medium & 1608 & 162 & 90\% \\ \hline
    Large & 359 & 82 & 77\% \\  \hline
    \hline
  \end{tabular}
\caption{Dell T410 with 8GB RAM:  Each Guest domain has 2GB Allocated. }
\label{fig:tps2}
\end{subtable}
\caption{Dom1 TPS (Transactions Per Second) for 3 database sizes, when run as a single VM and with 3 external guests running concurrently.}
\end{table}

\subsubsection{Existing tools analysis}
\indent Trying to analyze the performance drop in Dom1 without knowing about this external interference is difficult.  
There were no changes to Dom1 when run by itself and run with other guests.  
By using the system \emph{sar} utility we can examine available memory, SwapIn and BytesIn/s to see if we can determine why the application benchmark is degraded without collecting external information (Figure \ref{fig:vmstat}).  
Memory is not used as efficiently, the system almost completely elimiates swapping data in, and also can't read as quickly from disk.  
However, there is no indication that the problem was due to an external guest and hypervisor using those resources.
A DBA looking at these numbers may conclude that kernel swap or DB tuning may fix the problem.  
The root cause of the performance drop is due to external interference, which is not known from examining the data available.

\begin{table}[h]
  \begin{tabular}{ l | r | r | r }
    VMstat & Single & Multiple & Drop \\ \hline
	Memory & 211Kb & 138Kb & 35\% \\
	SwapIn/s & 1,480 & 85 & 94\% \\
	BytesIn/s & 6,877 & 4,438 & 35\% \\
  \end{tabular}
\caption{Some resource statistics collected with vmstat on Dom1 on a Large database while running alone and with multiple systems.} 
\label{fig:vmstat}
\end{table}

% Section 7.2
\subsection{Personal Server}
In the previous experiment we showed the interference that can occur when exteranl I/O interference is applied to a guest domain.
In this experiment, we repeat the previous previous experiment and pass kernel resource counter data between the hypervisor and domains.  
The results are from our IBM where each guest is configured to use 512MB of vRAM.
We run this against a \textbf{Medium DB} at 320MB in Dom1 and measure the I/O interference using the method in Section 5.  

\begin{Verbatim}
# xm list
Name                                        ID   Mem VCPUs      State   Time(s)
Domain-0                                     0  1988     4     r-----  88871.2
Test_VM1                                    25   512     1     -b----  60187.0
Test_VM2                                    26   512     1     -b----   5722.0
Test_VM3                                    27   512     1     -b----   5273.0
Test_VM4                                    28   512     1     -b----   5203.5
\end{Verbatim}

\subsubsection{Overhead}
We calculate the overhead for the guest OS paging and the I/O reads using the resource counters defined in our test suite.  While calculating the overhead we also collected the application performance at 260 TPS without and external interfernece.

\begin{table}[h]
\begin{tabular}{ l l l p{5cm} }
  Counter & Dom0 & Dom1 & $Overhead_V$ \\
  \hline
r sectors & 764,629 & 768,101 & 0\%  \\
\textbf{r\_ms} & 68,669 & 85,133 & \textbf{19\%} \\
r total   & 19,113 & 23,012 & 17\% \\
  \hline
\end{tabular}
\caption{I/O Overhead calculated average of four test runs on the IBM x3650.}
\label{fig:OverheadSmall}
\end{table}

From this table we can see that the \emph{r\_sectors} has a direct relation between the hypervisor and the guest VM.  The \emph{r\_ms} is a conter that we will use to determine the inteference from I/O since it tracks the time spent in the guest and hypervisor.  We calculate the virtualization overhead for this at 19\%.

\subsubsection{Memory Interference}
Since the DB size is 320MB and there is 512MB of vRAM on Dom1, we want to measure the interference when the working set can fit into memory.  
It is starting to switch from a memory bound system to an I/O system.
We repeat the previous experiment with 4 guest domains all running at the same time.
By using Equation 2 and 3 we can calculate the $Overhead_Vall$ and the $Interference$ for each counter.
While collecting the perormance counters, we also collected the application performance on DOM1 at 197 TPS.  A performance drop of 24\% when compared to running without any interfernce.

\begin{table}[h]
\begin{tabular}{ l l l l p{5cm} }
  Counter & Dom0 & $\sum{DomU}$ & $Overhead_Vall$ & $Interference$ \\
  \hline
       r sectors & 3,216,853 & 3,259,568 & \%    & 1\%\\
  \textbf{r\_ms} & 371,515 & 463,979 & \textbf{20\%} & \textbf{1\%} \\
       r total   & 49,306 & 64,668 & 24\%  & 7\% \\
  \hline
\end{tabular}
\caption{Interference calculated average of four test runs.}
\label{fig:InterferenceSm}
\end{table}

The \emph{r\_sectors} still shows a direct relation even when run with interference.  The emph{r\_ms} only shows a 1\% change in interference.  The total reads may not be related to the interference.  From these results we do not see I/O interference.  Or very little interference from I/O.  We suspect that it may be paging or memory management between the domains that causes 

\subsubsection{I/O Interference}
Since the previous experiment did not show very much interference from I/O we repeat that experiment with a larger database.  We change the database size in Dom1 - Dom4 to 640MB.  This is greater than the available ram and will cause significant I/O reads.



% Section 7.3
\subsection{Clearing Cache}
In this test we test our measurements on a system that does not have any stable cache. This will force most of
the reads to read from the disk IO. Before each test our driver script executes a command on the Linux guest
to drop the cache before running the benchmark. Each benchmark runs for 30 seconds.

\begin{Verbatim}
    sync
    echo 3 > /proc/sys/vm/drop_caches
    echo 0 > /proc/sys/vm/drop_caches
\end{Verbatim}

\indent For this experiment we use our Dell server with 8GB of physical RAM. We configure all Dom1 to use
4GB of vRAM and keep Dom2 - Dom4 at 2GB of vRAM. We create an I/O bound application by creating a
3 GB database in each domain. For this size of database expect to see a significant change in the application
when the external guest domains (Dom2 - Dom4) compete for physical resources with Dom1.

\begin{figure}[h]
\begin{Verbatim}
# xm list
Name                             ID   Mem VCPUs      State   Time(s)
Domain-0                          0   868    16     r-----  15615.0
TestVM1                           1   4096     2     -b----    129.0
TestVM2                           2   2048     2     -b----    106.9
TestVM3                           3   2048     2     -b----     83.7
TestVM4                           4   2048     2     -b----     91.8
\end{Verbatim}
\end{figure}

\subsubsection{Overhead}
For the overhead we use Equation 1 defined in the design to calculate the OverheadV for the I/O and virtual
memory counters. We monitor the counters in the guest machines and the hypervisor. Only TestVM1 runs
while calculating the overhead.

\begin{table}[h]
\begin{tabular}{ l l l p{5cm} }
  Counter & Dom0 & Dom1 & $Overhead_V$ \\
  \hline
pgpgin    & 1,704,366 & 851,643 & -100\% \\
pgpgout   & 4,925     & 1,751   & -181\% \\
pgfault   & 50,991    & 93,843  & 46\%   \\
r sectors & 1,704,425 & 1,703,286 & 0\%  \\
\textbf{r\_ms} & 110,385 & 144,668 & \textbf{24\%} \\
r total   & 41,182 & 48,988 & 16\% \\
  \hline
\end{tabular}
\caption{Overhead calculated average of four test runs.}
\label{fig:OverheadMed}
\end{table}

\subsubsection{Interference}
Now we repeat the experiment and collect the same counters in all guests and the hypervisor, while all guests are running. 
We collect the counters in the guest and the hypervisor for 30 seconds and stop and start the collection of counters between all guests and the hypervisor. 
We calculate the overhead from all guests $Overhead_V$ (Equation 2) by calculating the sum of the counters from all guests and comparing that to the counters collected from the hypervisor. 
Then we can calculate the $Interference$ (Equation 3) in the guest domain Dom1.

\begin{table}[h]
\begin{tabular}{ l l l l p{5cm} }
  Counter & Dom0 & $\sum{DomU}$ & $Overhead_V$ & $Interference$ \\
  \hline
pgpgin    & 6,411,078 & 3,201,715 & -100\% & 0\% \\
pgpgout   & 18,541     & 8,388   & -121\%  & 60\%\\
pgfault   & 386,319    & 407,700  & 46\%   & -40\% \\
r sectors & 1,704,425 & 1,703,286 & 0\%    & 0\%\\
\textbf{r\_ms} & 1,190,041 & 1,649,907 & \textbf{28\%} & \textbf{4\%} \\
r total   & 149,994 & 181,147 & 17\%  & 1\% \\
  \hline
\end{tabular}
\caption{Interference calculated average of four test runs.}
\label{fig:InterferenceMed}
\end{table}

Our only counter that measure time of a resource (r ms) was consistently greater than when we ran it
without interference. Although this does not account for all of the performance drop, it does contribute to
the performance issues and is consistent across all tests. This shows a 4\% interference while the application showed a 48\% drop in overall performance while running this set of tests.

From these results we can see that that the ratios are the same for the two counters that were directly
related when collecting the overhead. We can assume in this case that these counters are not effected by
interference, and our method shows a 0\% interference.   The total reads also remains fairly consistent although it is not directly related.  The pages out are too small in this case to make any conclusions.  We did not stress the writes when calculating the overhead, so pages out are probably not relevant for this case. 

Using our current calculation methods page faults shows a negative value, but it could explain the
additional interference. When only a single guest was running only about half of the page faults in the
guest were also run in the hypervisor. When run with all exteranal guest domains almost all of the page faults
resulted in page faults in the hypervisor.  Additional tests and analysis would need to be performed to see how page faults relate between the guests and the hypervisor.

% Section 7.4
\subsection{Verification without interference}
In the previous two experiments, we showed how our method can give a guest domain vital information when it is degraded from external interference.  However, what if the guest domain was degraded due to an application bug, DB change, or misconfiguration?   In that case we want our method to show that there is not any external interference.

\indent As shown previously, we use the fact that an application will degrade significantly from a memory bound system to an I/O bound system.  This is a real problem as DB can grow and needs to be purged or partitioned.  From our previous experiment with the Dell and Dom1 using 3GB vRAM.  We can use the overhead previously calculated and run 2 different tests.  The first test will use a 2GB database which will be bound by memory and will have a high TPS. The second test will have a 3GB database and will have have a much lower TPS.  Dom2 - Dom4 will be running but will not have any load or benchmark running (basically idle).








\begin{comment}
\begin{table}[h]
\begin{tabular}{ l l p{5cm} }
  Statistics & Description \\
  \hline
  /proc/diskstat & I/O statistics of block devices \\
  /proc/vmstat & Detailed virtual memory statistics\\
  \hline
\end{tabular}
\caption{Disk I/O performance counters on Linux.}
\label{tab:iocounters}
\end{table}

\subsection{}
Here we repeat the previous experiment with a large database, which is larger than available memory. We use the framework described in section 5.5 to collect I/O and virtual memory data from the /proc/diskstat and /proc/vmstat kernel statistics from all domains and the hypervisor.  From this data we can determine the impact of external I/O interference on our guest domain.\\
Before running the experiment, we need to calculate the I/O overhead by running the "dd" utility in a guest domain and collecting data without any interference.  After 3 runs we can see that the 


In this experiment we are a small virtual system: an IBM x3650 single socket quad core Intel Xeon CPU.   This machine runs CentOS 6.4 with a Xen configured kernel at 3.4.54 for Domain 0, and the Xen 4.2 hypervisor complied and packaged as part of the CentOS add on packages.   In this case Only the Dom0 is used and 3 separate memory configurations are used to show the difference between a system configured with 512MB, 1024MB, and 2048MB of ram.

\subsection{External Guest Interference}
In this experiment there we show the interference caused by running multiple virtual machines at the same time.  The following experiments are run with 4 CPUs and 2GB ruam divided between 4 virtual machines.  Each machine has 512MB and a single CPU.   In the Independent test all 4 tests are run at different times, while the concurrent test all 4 machines are run at the same time.  The reslts of the Physical machine are shown to show the Possible Maximum. 
Figure for the 512MB TPS is reported as the SUM of 4 machines.

\subsection{Guest Application Problem}
To similate a problem with a guest application we use use a misconfigured postgres database by changing the default blocksize to misalign with the guest OS ( and physical) page size.  
\textbf{--with-blocksize=BLOCKSIZE}
Set the block size, in kilobytes. This is the unit of storage and I/O within tables. The default, 8 kilobytes, is suitable for most situations; but other values may be useful in special cases. The value must be a power of 2 between 1 and 32 (kilobytes). Note that changing this value requires an initdb.  This will cause excessive reads and writes [40].  By default, the PG Block size is 8K and most (Linux) OS are configured to use 4k or 8k pages.   We use PGbench to generate an IO based workload which shows a performance degradation.  When the analyzer is run it will show a problem with the Guest Application.

\subsection{Cache Contention Problem}
Running multiple IO and memory intensive guests on the same core can result in LLC cache contention causing a performance degredation below the SLA.  PGbench can be run in a “select only” mode, in which most of the DB relations will be cached in memory.  If another DB guest is run on the same CPU socket the memory intesive application will interfere with the DB server [10].   We schedule two guest machines to run on CPU Socket 0 (core 0 and 1).  The analyzer detects the LLC cache interference and the DB server is significantly degraded below the SLA.  When moved onto separate sockets (socket 0 and 1) both the DB server and “Select Only” DB server run within the SLA.
4.6  I/O Contention 
In many cases multiple database servers could be shared on a system with multiple guests if each database only reaches peak throughput seldomly.  In this experiment we run 1 DB server per core and run the PGbench benchmark on each guest simultaneously.  If a single DB server can perform at x TPS, then it may be expected that 4 DB servers can operate at peak capacity at x/4 TPS.   Additionally, we can use PGreplay to see if we can run each system at a fraction of its speed (in this case 25\%) and achieve the same throughput on 4 separate guests.  Since there is additional overhead with the virtualiztion in DOM 0, there will be some overhead associated with the IO, and this will not be achievable.   Since a DB server synchronizes and journals the writes, each guest server will need to wait for the disk writes to complete.  The analyzer will detect the IO bottleneck issue and report problems.

\subsection{I/O Contention with ‘Disk Pinning’}
In virtualiztion it is possible to “Pin” a guest OS to a single core.  This is a way to make certain that each guest OS interference is known.  This may not be ideal in all cases, since the hypervisor may be able to better schedule guest machines on various cores (or give more cores to guests that need them).    An idea of disk pinning is similar and may (partially) alleviate some overhead in the previous experiment.  In this case, each physical disk is dedicated to a particular virtual machine, instead of a volume or raid group configured by the hypervisor.   Theoritcally, repeating the above experiment could come close to x/4 TPS except for memory cache contention.  Additionally each guest could possibly replay at 25\% of the optimal throughput on a system with 4 cores.    The issue would not be with the phyical disk I/O but with the overhead of Dom0 to schedule the disk writes.

\subsection{Virtual Cluster}
In this experiment we use a cluster of more than 1 server with a shared storage SAN server.  Each node needs an HBA with fiber connected to a fiber switch.   By running multiple guests with PG bench on the cluster server simultaneously, we can simulate an I/O contention with the shared backing store.   In a virtual cluster environment the analyzer should be able to determine that one or more guests on a particular server is exceeding their expected SLA.  Disk Pinning can also be analyzed with a large distributed backing store with multiple Logical disks (LUNs) connected with HBAs and virtualized to the guest systems.
\end{comment}
