In the following test experiments we evaluate the benefits of passing additional information through the layers of the hypervisor and guests for profiling.  Unless otherwise noted, each experiment is performed on a small and medium scale virtualization platform . See Figure \ref{fig:virtSize} for a description of the hardware and Figure \ref{fig:softStack} for the software stack installed.

\subsection{External machine interference}
In this experiment, we show the memory and I/O interference from external systems, without overcommitting the memory and CPU.  First we divide the available memory and CPUs into four equal parts and create four individual guest virtual machines (Dom1 - Dom4).  Each virtual machine is given 25\% of the availble resources so that no guest virtual machine would interfere with another machine if they were on separate physical systems.  We start our Dom1 machine and run our experimental test with PGbench.  Before each test, Linux pagecache and inodes are dropped to prevent previous caching from interfering with results.   While the benchmark is running, system data is collected from the virtual machine and hypervisor.  See Figure \ref{fig:memory}

Then we simulate external machine interference by running Dom1 concurrently with Dom2, Dom3, and Dom4.  In each machine size, we run the tests with 3 distinct database sizes.
\begin{description}
  \item[Small] The database is less than 1/2 available memory
  \item[Medium] The database is equal to available memory
  \item[Large] The database is 2 times available memory
\end{description}

On our small system the Dom1 is given 512MB ram and a single core.  When the system is memory bound there is about a 28\% drop in performance (4434 TPS - 3208 TPS), even though the application remains constant.  See Figure \ref{fig:tps1}.
From the OS layer, using tradtional utilites such as sar and vmstat, the guest OS can see that there is (5\%) less free memory for the application, but that seems to not be used by the application or OS cache which also drops by 5\%.  Additionally the CPU spends more than twice as long waiting on Disk I/O.
\newline
Similar results are seen when the system is IO bound with a 24\% drop in TPS and 35\% drop in free memory.  The most significant change is in the Medium size which results in a 90\% performance decrease.  This is because the application changes from a memory bound application to an I/O bound application when there is contention from external systems for memory resources.  

\begin{figure}
  \begin{tabular}{ l | r | r | r }
    Size & Single & Multiple & Drop \\
    \hline
    Small & 4434 & 3208 & 28\% \\ \hline
    Medium & 2149 & 216 & 90\% \\ \hline
    Large & 260 & 197 & 24\% \\  \hline
  \end{tabular}
\caption{Dom1 TPS (Transactions Per Second for 3 database sizes, and the changes when run as a single VM or with multiple VMs running simultaneously.}
\label{fig:tps1}
\end{figure}

Another interesting result from this experiment is that the Guest OS does not swap when the memory pressure is high from other systems.  The Large database which is larger than main pages memory in and out as needed based on the kernel \emph{swappiness} setting.  However, when the Dom1 runs concurrently with the other systems, it swaps very little or not at all.
\begin{figure}
  \begin{tabular}{ l | r | r | r }
    Vmstat & Memory & SwapIn/s & BytesIn/s \\
    \hline
    Single & 211Kb &  1,480 & 6,877 \\ \hline
    Multiple & 138Kb & 85 & 4,438 \\ \hline
    Drop & 35\% & 94\% & 35\% \\  \hline
  \end{tabular}
\caption{Some output of vmstat on a Large database while running alone and with multiple systems} 
\label{fig:vmstat}
\end{figure}

\begin{comment}
\subsection{Detecting Memory and I/O contention}
In this experiment we are a small virtual system: an IBM x3650 single socket quad core Intel Xeon CPU.   This machine runs CentOS 6.4 with a Xen configured kernel at 3.4.54 for Domain 0, and the Xen 4.2 hypervisor complied and packaged as part of the CentOS add on packages.   In this case Only the Dom0 is used and 3 separate memory configurations are used to show the difference between a system configured with 512MB, 1024MB, and 2048MB of ram.

\subsection{External Guest Interference}
In this experiment there we show the interference caused by running multiple virtual machines at the same time.  The following experiments are run with 4 CPUs and 2GB ruam divided between 4 virtual machines.  Each machine has 512MB and a single CPU.   In the Independent test all 4 tests are run at different times, while the concurrent test all 4 machines are run at the same time.  The reslts of the Physical machine are shown to show the Possible Maximum. 
Figure for the 512MB TPS is reported as the SUM of 4 machines.

\subsection{Guest Application Problem}
To similate a problem with a guest application we use use a misconfigured postgres database by changing the default blocksize to misalign with the guest OS ( and physical) page size.  
\textbf{--with-blocksize=BLOCKSIZE}
Set the block size, in kilobytes. This is the unit of storage and I/O within tables. The default, 8 kilobytes, is suitable for most situations; but other values may be useful in special cases. The value must be a power of 2 between 1 and 32 (kilobytes). Note that changing this value requires an initdb.  This will cause excessive reads and writes [40].  By default, the PG Block size is 8K and most (Linux) OS are configured to use 4k or 8k pages.   We use PGbench to generate an IO based workload which shows a performance degradation.  When the analyzer is run it will show a problem with the Guest Application.

\subsection{Cache Contention Problem}
Running multiple IO and memory intensive guests on the same core can result in LLC cache contention causing a performance degredation below the SLA.  PGbench can be run in a “select only” mode, in which most of the DB relations will be cached in memory.  If another DB guest is run on the same CPU socket the memory intesive application will interfere with the DB server [10].   We schedule two guest machines to run on CPU Socket 0 (core 0 and 1).  The analyzer detects the LLC cache interference and the DB server is significantly degraded below the SLA.  When moved onto separate sockets (socket 0 and 1) both the DB server and “Select Only” DB server run within the SLA.
4.6  I/O Contention 
In many cases multiple database servers could be shared on a system with multiple guests if each database only reaches peak throughput seldomly.  In this experiment we run 1 DB server per core and run the PGbench benchmark on each guest simultaneously.  If a single DB server can perform at x TPS, then it may be expected that 4 DB servers can operate at peak capacity at x/4 TPS.   Additionally, we can use PGreplay to see if we can run each system at a fraction of its speed (in this case 25\%) and achieve the same throughput on 4 separate guests.  Since there is additional overhead with the virtualiztion in DOM 0, there will be some overhead associated with the IO, and this will not be achievable.   Since a DB server synchronizes and journals the writes, each guest server will need to wait for the disk writes to complete.  The analyzer will detect the IO bottleneck issue and report problems.

\subsection{I/O Contention with ‘Disk Pinning’}
In virtualiztion it is possible to “Pin” a guest OS to a single core.  This is a way to make certain that each guest OS interference is known.  This may not be ideal in all cases, since the hypervisor may be able to better schedule guest machines on various cores (or give more cores to guests that need them).    An idea of disk pinning is similar and may (partially) alleviate some overhead in the previous experiment.  In this case, each physical disk is dedicated to a particular virtual machine, instead of a volume or raid group configured by the hypervisor.   Theoritcally, repeating the above experiment could come close to x/4 TPS except for memory cache contention.  Additionally each guest could possibly replay at 25\% of the optimal throughput on a system with 4 cores.    The issue would not be with the phyical disk I/O but with the overhead of Dom0 to schedule the disk writes.

\subsection{Virtual Cluster}
In this experiment we use a cluster of more than 1 server with a shared storage SAN server.  Each node needs an HBA with fiber connected to a fiber switch.   By running multiple guests with PG bench on the cluster server simultaneously, we can simulate an I/O contention with the shared backing store.   In a virtual cluster environment the analyzer should be able to determine that one or more guests on a particular server is exceeding their expected SLA.  Disk Pinning can also be analyzed with a large distributed backing store with multiple Logical disks (LUNs) connected with HBAs and virtualized to the guest systems.
\end{comment}
